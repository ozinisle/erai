{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Predictor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "- You will have to pocess the stock's data in json format in a folder location which istrictly follows the following pattern \n",
    "    <b>'data/***stockname***/raw/***type-of-data***'</b>\n",
    "    \n",
    "    For example, <b>'data/itc/raw/1h' </b>\n",
    "    \n",
    "- The folder location that you will mention below should have json files with data associated with only one stock information. If not the please clean it up and until then do not proceed further with this procedure\n",
    "\n",
    "- You Will have to ensure that the data provided via the json files are clean \n",
    "    - should not have duplicates \n",
    "    - should not have incomplete, empty or semi furnished information\n",
    "    - should not have wrong or assumed data which might screw the predictor. \n",
    "    \n",
    "- You will have to configure the following inputs to kick start the prediction process\n",
    "\n",
    "- <b><u>CAUTION</u></b> : The procedure will create a folder named \"processed\" parallel to the folder named\"raw\" as mentioned in the input format. If you have such a folder already in place <b><u> BE WARNED - IT WILL BE OVER WRITTEN </u></b>. You have been warned.\n",
    "\n",
    "The following procedure does not have any explict check or validation with respect to the items mentioned above\n",
    "\n",
    "\n",
    "## Configure inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "raw_data_folder_path = 'data/adani/raw/1h' \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kick start the procedure - Collate all json data into a CSV file\n",
    "\n",
    "Process all json data files in folder mentioned above and and create a cummulative data file in csv format with desirable column headers to help differnentiate or understand the data during further processing in the folder named \"processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Include python files defined in the library folder to help facilitate the stock price Prediction \n",
    "import os\n",
    "import sys  \n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "sys.path.insert(0, module_path+'\\\\lib')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looping through all the files to create input data\n",
      "Attempting to create folder if it does not exist >>>C:\\Users\\kalagi01\\Desktop\\dev\\erai\\data\\adani\\processed\n",
      "Attempting to create/update file >>>C:\\Users\\kalagi01\\Desktop\\dev\\erai\\data\\adani\\processed/processedRawData.csv\n",
      "created raw easy to use csv data to be used for preparing training data in the location  >>>C:\\Users\\kalagi01\\Desktop\\dev\\erai\\data\\adani\\processed/processedRawData.csv\n",
      "\n",
      "processed raw data and created processable csv file for data preparation at >>>C:\\Users\\kalagi01\\Desktop\\dev\\erai\\data\\adani\\processed/processedRawData.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from dataPreparation import processRawData\n",
    "rawProcessedInputDataInfo=processRawData(raw_data_folder_path)\n",
    "print('\\nprocessed raw data and created processable csv file for data preparation at >>>' + rawProcessedInputDataInfo[1])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dataPreparation import createInputData\n",
    "\n",
    "#rawDataFilePath=rawProcessedInputDataInfo[1]\n",
    "rawDataFilePath=\"C:\\\\Users\\\\kalagi01\\\\Desktop\\\\dev\\\\erai\\\\data\\\\adani\\\\processed\\\\processedRawData.csv\"\n",
    "input_data_preparation_response = createInputData(rawDataFilePath)\n",
    "preparedTrainingDataDF=input_data_preparation_response[3]\n",
    "print('done')\n",
    "preparedTrainingDataDF.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfInputFeatures = preparedTrainingDataDF.shape[1]\n",
    "numberOfOutputFeatures = 5\n",
    "\n",
    "numberOfInputFeatures,numberOfOutputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(preparedTrainingDataDF[\"open\"])\n",
    "plt.plot(preparedTrainingDataDF[\"high\"])\n",
    "plt.plot(preparedTrainingDataDF[\"low\"])\n",
    "plt.plot(preparedTrainingDataDF[\"close\"])\n",
    "plt.title('Stock price history')\n",
    "plt.ylabel('Price (Rupees)')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(['open','high','low','close'], loc='upper left')\n",
    "plt.show()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr = preparedTrainingDataDF.corr()\n",
    "print('done')\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
    "\n",
    "def magnify():\n",
    "    return [dict(selector=\"th\",\n",
    "                 props=[(\"font-size\", \"7pt\")]),\n",
    "            dict(selector=\"td\",\n",
    "                 props=[('padding', \"0em 0em\")]),\n",
    "            dict(selector=\"th:hover\",\n",
    "                 props=[(\"font-size\", \"12pt\")]),\n",
    "            dict(selector=\"tr:hover td:hover\",\n",
    "                 props=[('max-width', '200px'),\n",
    "                        ('font-size', '12pt')])\n",
    "]\n",
    "\n",
    "corr.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
    "    .set_caption(\"Hover to magify\")\\\n",
    "    .set_precision(2)\\\n",
    "    .set_table_styles(magnify())\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(corr,   xticklabels=corr.columns,     yticklabels=corr.columns)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainingDataFilePath = input_data_preparation_response[1]\n",
    "trainingDataFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from forecast import *\n",
    "success, trainingData, min_max_scaler, x_t, y_t,x_test_t, y_test_t, lstm_model, history = trainLSTMModel(trainingDataFilePath, TIME_STEPS_CONFIG=60, BATCH_SIZE_CONFIG = 512, LEARNING_RATE_CONFIG=0.001, EPOCHS_CONFIG = 20)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save trained model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "modelFolderPath=getParentFolder(trainingDataFilePath)+\"\\model\"\n",
    "modelFilePath = modelFolderPath+\"\\lstm_model.sav\"\n",
    "createFolder(modelFolderPath)\n",
    "\n",
    "pickle.dump(lstm_model, open(modelFilePath, 'wb'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "lstm_model_loaded = pickle.load(open(modelFilePath, 'rb'))\n",
    "result = lstm_model_loaded.evaluate(x_t, y_t, batch_size=512)\n",
    "actual = lstm_model.evaluate(x_t, y_t, batch_size=512)\n",
    "print('done')\n",
    "actual, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lstm_model_loaded.evaluate(x_test_t, y_test_t, batch_size=512)\n",
    "actual = lstm_model.evaluate(x_test_t, y_test_t, batch_size=512)\n",
    "print('done')\n",
    "actual, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t.shape,x_test_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_values = lstm_model_loaded.predict(x_test_t, batch_size=512)\n",
    "predicted_values.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols_data = np.random.random((predicted_values.shape[0],numberOfInputFeatures-numberOfOutputFeatures))\n",
    "predicted_values_adj = np.append(predicted_values, dummy_cols_data, 1)\n",
    "predicted_values_orig_scale_withDummies=min_max_scaler.inverse_transform(predicted_values_adj)\n",
    "\n",
    "y_test_t_adj = np.append(y_test_t, dummy_cols_data, 1)\n",
    "y_test_t_orig_scale_withDummies=min_max_scaler.inverse_transform(y_test_t_adj)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values_orig_scale= predicted_values_orig_scale_withDummies[:, [1,2,3,4,5]]\n",
    "y_test_t_orig_scale= y_test_t_orig_scale_withDummies[:, [1,2,3,4,5]]\n",
    "predicted_values_orig_scale.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.clf()\n",
    "predicted_open = predicted_values_orig_scale[:,[0]]\n",
    "actual_open = y_test_t_orig_scale[:,[0]]\n",
    "\n",
    "x_coord = [ index for index in range(predicted_values_orig_scale.shape[0]) ]\n",
    "\n",
    "plt.plot(x_coord, predicted_open, 'g', label='Predicted Open')\n",
    "plt.plot(x_coord, actual_open, 'y', label='Actual Open')\n",
    "plt.title('Open -> Predicted vs Actual')\n",
    "plt.xlabel('Range')\n",
    "plt.ylabel('Open Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values_orig_scale.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
